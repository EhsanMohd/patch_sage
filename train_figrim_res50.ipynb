{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e238f100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrahm326/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21 classes · 508 train images · 128 val images\n"
     ]
    }
   ],
   "source": [
    "# Load \n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse, time, os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Load \n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse, time, os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "\n",
    "# Define preprocessing transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to model input size\n",
    "    transforms.ToTensor(),          # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define the dataset\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "seed = 42\n",
    "root = \"/home/mrahm326/experiments/Probing Mechanism/figrim/Targets\"\n",
    "\n",
    "# Your existing train transform (from your snippet)\n",
    "train_transform = transform\n",
    "\n",
    "# Simple eval transform (adjust to your preprocessing: size/normalize, etc.)\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load once without transforms to read labels for a stratified split\n",
    "full_base = datasets.ImageFolder(root=root)\n",
    "y = np.array(full_base.targets)\n",
    "idx = np.arange(len(y))\n",
    "\n",
    "# Stratified split (keeps class balance)\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n",
    "train_idx, test_idx = next(sss.split(idx, y))\n",
    "\n",
    "# Build separate datasets so train/test can have different transforms\n",
    "train_dataset = Subset(datasets.ImageFolder(root=root, transform=train_transform), train_idx.tolist())\n",
    "test_dataset  = Subset(datasets.ImageFolder(root=root, transform=eval_transform),  test_idx.tolist())\n",
    "\n",
    "# (Optional) DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True,  num_workers=4, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import efficientnet_b0\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "num_classes = len(full_base.classes)\n",
    "print(f\"Found {num_classes} classes · {len(train_dataset)} train images · {len(test_dataset)} val images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29c261ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded patch shape: torch.Size([1, 3, 11, 11]), requires_grad: True, device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2078397/2256496505.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  adv_patch = torch.load('/home/mrahm326/experiments/Probing Mechanism/DeepGaze/Figrim_ALL/adversarial_patch_figrim_eff.pth', map_location=device)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set device first\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assume adv_patch is a tensor\n",
    "def create_patch(patch_size=0.2, image_size=224):\n",
    "    patch_dim = int(patch_size * image_size)\n",
    "    patch = torch.rand(3, patch_dim, patch_dim, requires_grad=True, device=device)\n",
    "    return patch\n",
    "#torch.save(adv_patch, 'adversarial_patch.pth')\n",
    "adv_patch = create_patch(patch_size=0.2, image_size=224)\n",
    "import torch\n",
    "\n",
    "# Load the saved patch\n",
    "adv_patch = torch.load('/home/mrahm326/experiments/Probing Mechanism/DeepGaze/Figrim_ALL/adversarial_patch_figrim_eff.pth', map_location=device)\n",
    "\n",
    "# If you need it to require gradients for optimization or testing:\n",
    "adv_patch.requires_grad = True\n",
    "\n",
    "# Confirm shape and device\n",
    "print(f\"Loaded patch shape: {adv_patch.shape}, requires_grad: {adv_patch.requires_grad}, device: {adv_patch.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6ae671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2078397/1668887239.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sal_model.load_state_dict(torch.load(\"/home/mrahm326/experiments/Probing Mechanism/DeepGaze/best_deeplabv3_saliency_cat2000.pt\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nimport torch.optim as optim\\n\\noptimizer = optim.Adam(\\n    model.parameters(), \\n    lr=1e-4,          # learning rate\\n    betas=(0.9, 0.999),\\n    weight_decay=1e-4 # L2 regularization\\n)\\ncriterion = nn.CrossEntropyLoss()\\n\\nnum_epochs = 6\\nfor epoch in range(num_epochs):\\n    model.train()\\n    total_loss, total_correct, total_samples = 0.0, 0, 0\\n\\n    for images, labels in train_loader:\\n        images, labels = images.to(device), labels.to(device)\\n        optimizer.zero_grad()\\n\\n        outputs = model(images)\\n        loss = criterion(outputs, labels)\\n        loss.backward()\\n        optimizer.step()\\n\\n        total_loss += loss.item()\\n        total_correct += (outputs.argmax(1) == labels).sum().item()\\n        total_samples += labels.size(0)\\n\\n    accuracy = 100. * total_correct / total_samples\\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}, Accuracy: {accuracy:.2f}%\")\\n\\n    # === Validation (optional) ===\\n    model.eval()\\n    val_correct = 0\\n    with torch.no_grad():\\n        for images, labels in test_loader:\\n            images, labels = images.to(device), labels.to(device)\\n            outputs = model(images)\\n            val_correct += (outputs.argmax(1) == labels).sum().item()\\n    val_accuracy = 100. * val_correct / len(test_dataset)\\n    print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\\n\\n# === Save Model ===\\n# \\ntorch.save(model.state_dict(), \"best_res50_figrim.pt\")\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sal_model = models.segmentation.deeplabv3_resnet50(weights=None, num_classes=1)  # Single channel output\n",
    "\n",
    "from torchvision import models\n",
    "\n",
    "# Load pretrained model\n",
    "sal_model = models.segmentation.deeplabv3_resnet50(weights=models.segmentation.DeepLabV3_ResNet50_Weights.DEFAULT)\n",
    "\n",
    "# Replace the classifier to output a single-channel saliency map\n",
    "sal_model.classifier[-1] = torch.nn.Conv2d(256, 1, kernel_size=1)\n",
    "# Load the best trained model\n",
    "sal_model.load_state_dict(torch.load(\"/home/mrahm326/experiments/Probing Mechanism/DeepGaze/best_deeplabv3_saliency_cat2000.pt\"))\n",
    "sal_model.eval()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pretrained ViT\n",
    "from torchvision import models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Pretrained EfficientNet-B0\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Replace the final fully connected (fc) layer\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "# Move to device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# NOTE: This checkpoint is for a ViT and WON'T load into EfficientNet.\n",
    "# Use an EfficientNet checkpoint trained with the same architecture instead.\n",
    "# Example (if you have one):\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=1e-4,          # learning rate\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=1e-4 # L2 regularization\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 6\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    accuracy = 100. * total_correct / total_samples\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    # === Validation (optional) ===\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "    val_accuracy = 100. * val_correct / len(test_dataset)\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "# === Save Model ===\n",
    "\n",
    "torch.save(model.state_dict(), \"best_res50_figrim.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39942f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2078397/2733081973.py:164: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  adv_patch = torch.load(\"adversarial_patch_figrim_res50.pth\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import random, os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- Config ----------\n",
    "\n",
    "batch_size = 32\n",
    "patch_size = 0.05  # size relative to image (e.g., 0.2 → 20% of image height/width)\n",
    "targeted = True\n",
    "target_class = 0  # only used in targeted mode\n",
    "attack_steps = 300\n",
    "attack_lr = 5e-2\n",
    "\n",
    "# ---------- Data Transforms ----------\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "# ---- helpers (put near your config) ----\n",
    "mean_t = torch.tensor([0.485, 0.456, 0.406], device=device).view(3,1,1)\n",
    "std_t  = torch.tensor([0.229, 0.224, 0.225], device=device).view(3,1,1)\n",
    "\n",
    "def to_model_space(x01):    # [0,1] -> normalized\n",
    "    return (x01 - mean_t) / std_t\n",
    "\n",
    "def to_pixel_space(xnorm):  # normalized -> [0,1]\n",
    "    return torch.clamp(xnorm * std_t + mean_t, 0, 1)\n",
    "\n",
    "# ---- Patch param & differentiable application ----\n",
    "def create_patch(patch_size=0.2, image_size=224):\n",
    "    ph = int(round(patch_size * image_size))\n",
    "    # Unconstrained parameter; we'll pass it through sigmoid each step\n",
    "    return nn.Parameter(torch.zeros(3, ph, ph, device=device))\n",
    "\n",
    "def apply_patch(imgs, patch_norm):\n",
    "    \"\"\"\n",
    "    imgs: normalized images [B,3,H,W]\n",
    "    patch_norm: normalized patch [3,ph,pw]\n",
    "    returns: patched images (differentiable w.r.t. patch_norm)\n",
    "    \"\"\"\n",
    "    B, C, H, W = imgs.shape\n",
    "    ph, pw = patch_norm.shape[1:]\n",
    "    masks = torch.zeros((B, 1, H, W), device=imgs.device)\n",
    "    for i in range(B):\n",
    "        top = random.randint(0, H - ph)\n",
    "        left = random.randint(0, W - pw)\n",
    "        masks[i, :, top:top+ph, left:left+pw] = 1.0\n",
    "    # broadcast patch across spatial dims, mask selects the region\n",
    "    patched = imgs * (1 - masks) + patch_norm.unsqueeze(0) * masks\n",
    "    return patched\n",
    "\n",
    "\n",
    "def apply_patch(imgs, patch_norm):\n",
    "    \"\"\"\n",
    "    imgs: normalized images [B,3,H,W]\n",
    "    patch_norm: normalized patch [3, ph, pw]\n",
    "    returns: [B,3,H,W], differentiable w.r.t. patch_norm\n",
    "    \"\"\"\n",
    "    B, C, H, W = imgs.shape\n",
    "    ph, pw = patch_norm.shape[1:]\n",
    "    out = []\n",
    "\n",
    "    for _ in range(B):\n",
    "        top  = random.randint(0, H - ph)\n",
    "        left = random.randint(0, W - pw)\n",
    "        # Pad patch into a full-size canvas at (top,left)\n",
    "        # F.pad pads (left, right, top, bottom) on the last two dims.\n",
    "        pad_left, pad_right  = left, W - left - pw\n",
    "        pad_top,  pad_bottom = top,  H - top  - ph\n",
    "        patch_canvas = F.pad(patch_norm.unsqueeze(0), (pad_left, pad_right, pad_top, pad_bottom))  # [1,3,H,W]\n",
    "        patch_canvas = patch_canvas.squeeze(0)  # [3,H,W]\n",
    "\n",
    "        # Build a 3-channel mask matching the placed region\n",
    "        mask = torch.zeros((1, H, W), device=imgs.device)\n",
    "        mask[:, top:top+ph, left:left+pw] = 1.0\n",
    "        mask3 = mask.expand(C, -1, -1)  # [3,H,W]\n",
    "\n",
    "        out.append(imgs[0] * (1 - mask3) + patch_canvas)  # blend\n",
    "        imgs = imgs[1:]  # advance to next image\n",
    "\n",
    "    return torch.stack(out, dim=0)\n",
    "\n",
    "\n",
    "def generate_adversarial_patch(model, loader, patch_size=0.2, steps=300, lr=5e-2,\n",
    "                               targeted=True, target_class=0, image_size=224):\n",
    "    # parameterize the patch\n",
    "    patch_param = create_patch(patch_size, image_size)\n",
    "    optimizer = torch.optim.Adam([patch_param], lr=lr)\n",
    "\n",
    "    model.eval()\n",
    "    # (optional) freeze model weights; grads will flow to inputs/patch only\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad_(False)\n",
    "\n",
    "    for step in tqdm(range(steps), desc=\"Optimizing Patch\"):\n",
    "        total_loss = 0.0\n",
    "        for imgs, labels in loader:\n",
    "            imgs = imgs.to(device)\n",
    "\n",
    "            # param -> [0,1] -> normalized for the model\n",
    "            patch_01   = torch.sigmoid(patch_param)\n",
    "            patch_norm = to_model_space(patch_01)\n",
    "\n",
    "            patched_imgs = apply_patch(imgs, patch_norm)\n",
    "            outputs = model(patched_imgs)\n",
    "\n",
    "            if targeted:\n",
    "                target = torch.full((imgs.size(0),), target_class, dtype=torch.long, device=device)\n",
    "                loss = F.cross_entropy(outputs, target)\n",
    "            else:\n",
    "                loss = -F.cross_entropy(outputs, labels.to(device))\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Step {step}/{steps}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "    # return patch in [0,1] space for saving/inspection\n",
    "    with torch.no_grad():\n",
    "        return torch.sigmoid(patch_param).detach()\n",
    "\n",
    "def evaluate_patch_success(model, loader, patch01, targeted=True, target_class=0):\n",
    "    total = 0\n",
    "    fooled = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        patch_norm = to_model_space(patch01.to(device))\n",
    "        for imgs, labels in loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            patched_imgs = apply_patch(imgs, patch_norm)\n",
    "            preds = model(patched_imgs).argmax(dim=1)\n",
    "            fooled += (preds == target_class).sum().item() if targeted else (preds != labels).sum().item()\n",
    "            total += imgs.size(0)\n",
    "    print(f'{\"Targeted\" if targeted else \"Untargeted\"} Attack Success Rate: {fooled/total:.2%}')\n",
    "\n",
    "def visualize_patch(patch01):\n",
    "    # patch01 is in [0,1]\n",
    "    save_image(patch01, \"adv_patch.png\")\n",
    "    print(\"Adversarial patch saved to adv_patch.png\")\n",
    "    plt.imshow(np.transpose(patch01.detach().cpu().numpy(), (1, 2, 0)))\n",
    "    plt.title(\"Adversarial Patch\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "adv_patch = generate_adversarial_patch(\n",
    "    model, test_loader, patch_size, attack_steps, attack_lr, targeted, target_class, image_size=224\n",
    ")\n",
    "evaluate_patch_success(model, test_loader, adv_patch, targeted, target_class)\n",
    "visualize_patch(adv_patch)\n",
    "torch.save(adv_patch, 'adversarial_patch_figrim_res50.pth')\n",
    "\n",
    "adv_patch = torch.load(\"adversarial_patch_figrim_res50.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da697d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.ndimage import zoom\n",
    "from scipy.special import logsumexp\n",
    "import pickle\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Config ===\n",
    "NUM_CLASSES = num_classes \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# === Hooks ===\n",
    "def get_hook(storage, name):\n",
    "    def hook(module, input, output):\n",
    "        storage[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "block_modules = [(name, module) for name, module in model.named_modules() if isinstance(module, torch.nn.Conv2d)]\n",
    "layer_names = [name for name, _ in block_modules]\n",
    "\n",
    "def compute_iou(mask1, mask2):\n",
    "    intersection = np.logical_and(mask1, mask2).sum()\n",
    "    union = np.logical_or(mask1, mask2).sum()\n",
    "    return intersection / union if union else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a2c5c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_patch_random_location(img, adv_patch):\n",
    "    \"\"\"\n",
    "    Apply adversarial patch at a random location for each image in the batch.\n",
    "    Args:\n",
    "        img: Tensor of shape [B, C, H, W]\n",
    "        adv_patch: Tensor of shape [C, ph, pw] or [1, C, ph, pw]\n",
    "    Returns:\n",
    "        Tensor with patches applied\n",
    "    \"\"\"\n",
    "    patched = img.clone()\n",
    "    B, C, H, W = img.shape\n",
    "\n",
    "    # If adv_patch is batched, squeeze it\n",
    "    if adv_patch.dim() == 4:\n",
    "        adv_patch = adv_patch[0]\n",
    "\n",
    "    _, ph, pw = adv_patch.shape  # now always (C, ph, pw)\n",
    "\n",
    "    for i in range(B):\n",
    "        top = torch.randint(0, H - ph + 1, (1,)).item()\n",
    "        left = torch.randint(0, W - pw + 1, (1,)).item()\n",
    "        patched[i, :, top:top+ph, left:left+pw] = adv_patch\n",
    "\n",
    "    return patched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc98d001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.ndimage import zoom\n",
    "from scipy.special import logsumexp\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Config ===\n",
    "NUM_CLASSES = num_classes\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "features_by_class = {i: [] for i in range(NUM_CLASSES)}\n",
    "labels_by_class = {i: [] for i in range(NUM_CLASSES)}\n",
    "iou_diffs_by_class = {i: [] for i in range(NUM_CLASSES)}\n",
    "\n",
    "# === Hooks ===\n",
    "def get_hook(storage, name):\n",
    "    def hook(module, input, output):\n",
    "        storage[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "block_modules = [(name, module) for name, module in model.named_modules() if isinstance(module, torch.nn.Conv2d)]\n",
    "layer_names = [name for name, _ in block_modules]\n",
    "\n",
    "def collect_activation_norms(model, img, layer_names):\n",
    "    activations = {}\n",
    "    hooks = [dict(model.named_modules())[name].register_forward_hook(get_hook(activations, name)) for name in layer_names]\n",
    "    _ = model(img)\n",
    "    for h in hooks: h.remove()\n",
    "    act_norms = [torch.norm(activations[name]).item() if name in activations else 0.0 for name in layer_names]\n",
    "    return act_norms\n",
    "\n",
    "# === Layer Sensitivity Function ===\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_sensitivities(model, img, layer_names):\n",
    "    model.eval()\n",
    "    activations = {}\n",
    "\n",
    "    # Get clean output first\n",
    "    with torch.no_grad():\n",
    "        clean_output = model(img)\n",
    "        clean_conf = F.softmax(clean_output, dim=1)\n",
    "\n",
    "    # Register hooks to capture original activations\n",
    "    hooks = [dict(model.named_modules())[name].register_forward_hook(get_hook(activations, name)) for name in layer_names]\n",
    "    _ = model(img)\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    sensitivities = []\n",
    "\n",
    "    for name in layer_names:\n",
    "        if name not in activations:\n",
    "            sensitivities.append(0.0)\n",
    "            continue\n",
    "\n",
    "        original_act = activations[name]\n",
    "        noise = torch.randn_like(original_act) * 0.1\n",
    "        perturbed_act = original_act + noise\n",
    "\n",
    "        def override_hook(module, input, output):\n",
    "            return perturbed_act\n",
    "\n",
    "        # Register perturbation hook\n",
    "        temp_hook = dict(model.named_modules())[name].register_forward_hook(override_hook)\n",
    "        pert_output = model(img)\n",
    "        pert_conf = F.softmax(pert_output, dim=1)\n",
    "        temp_hook.remove()\n",
    "\n",
    "        # Now delta is the effect of perturbing `name`-th layer on the output confidence\n",
    "        delta = torch.norm(pert_conf - clean_conf).item()\n",
    "        sensitivities.append(delta)\n",
    "\n",
    "    #print(sensitivities)\n",
    "    return sensitivities\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_dataset_sensitivities(model, dataloader, layer_names, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    total_sensitivities = [0.0 for _ in layer_names]\n",
    "    count = 0\n",
    "\n",
    "    for imgs, _ in tqdm(dataloader, desc=\"Computing sensitivities\"):\n",
    "        imgs = imgs.to(device)\n",
    "        batch_size = imgs.size(0)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            img = imgs[i].unsqueeze(0)  # single image\n",
    "            sensitivities = compute_sensitivities(model, img, layer_names)\n",
    "            total_sensitivities = [total + s for total, s in zip(total_sensitivities, sensitivities)]\n",
    "            count += 1\n",
    "\n",
    "    avg_sensitivities = [s / count for s in total_sensitivities]\n",
    "    return avg_sensitivities\n",
    "\n",
    "# === Main Loop ===\n",
    "\n",
    "sens = compute_dataset_sensitivities(model, train_loader, layer_names, DEVICE)\n",
    "print(sens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d9bc77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41f71e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 21\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset, ConcatDataset\n",
    "import torch\n",
    "\n",
    "def get_num_classes_from_loader(loader):\n",
    "    ds = loader.dataset\n",
    "\n",
    "    # unwrap Subset(s)\n",
    "    while isinstance(ds, Subset):\n",
    "        ds = ds.dataset\n",
    "\n",
    "    # handle ConcatDataset by merging child datasets' class info\n",
    "    if isinstance(ds, ConcatDataset):\n",
    "        class_counts = []\n",
    "        for child in ds.datasets:\n",
    "            # unwrap nested Subsets inside the concat\n",
    "            while isinstance(child, Subset):\n",
    "                child = child.dataset\n",
    "            class_counts.append(_num_classes_from_dataset(child, loader=None))\n",
    "        # assume same label space across children; fall back to max\n",
    "        return max(class_counts)\n",
    "\n",
    "    return _num_classes_from_dataset(ds, loader)\n",
    "\n",
    "def _num_classes_from_dataset(ds, loader=None):\n",
    "    # Common, fast paths\n",
    "    if hasattr(ds, \"classes\"):          # e.g. ImageFolder\n",
    "        return len(ds.classes)\n",
    "    if hasattr(ds, \"class_to_idx\"):     # mapping dict\n",
    "        return len(ds.class_to_idx)\n",
    "    if hasattr(ds, \"targets\"):          # e.g. CIFAR, MNIST\n",
    "        return len(set(map(int, ds.targets)))\n",
    "    if hasattr(ds, \"labels\"):           # some custom datasets\n",
    "        return len(set(map(int, ds.labels)))\n",
    "\n",
    "    # Last resort: scan labels from a loader or the dataset itself\n",
    "    labels_seen = set()\n",
    "    if loader is not None:\n",
    "        for _, y in loader:\n",
    "            y = y if torch.is_tensor(y) else torch.tensor(y)\n",
    "            labels_seen.update(y.view(-1).tolist())\n",
    "    else:\n",
    "        for i in range(len(ds)):\n",
    "            _, y = ds[i]\n",
    "            y = y if torch.is_tensor(y) else torch.tensor(y)\n",
    "            labels_seen.add(int(y.item()))\n",
    "    return len(labels_seen)\n",
    "\n",
    "# Usage\n",
    "num_classes = get_num_classes_from_loader(train_loader)\n",
    "print(\"Number of classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeed9994",
   "metadata": {},
   "source": [
    "### Acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926ce2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.ndimage import zoom\n",
    "from scipy.special import logsumexp\n",
    "import pickle\n",
    "\n",
    "# === Config ===\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "unique_labels = set()\n",
    "for _, lbl in train_loader:\n",
    "    unique_labels.update(lbl.tolist())\n",
    "features_by_class = {int(k): [] for k in sorted(unique_labels)}\n",
    "labels_by_class   = {int(k): [] for k in sorted(unique_labels)}\n",
    "iou_diffs_by_class= {int(k): [] for k in sorted(unique_labels)}\n",
    "\n",
    "\n",
    "all_labels = []\n",
    "for _, lbl in train_loader:\n",
    "    all_labels.extend(lbl.tolist())\n",
    "print(\"Unique labels:\", sorted(set(all_labels))[:30])\n",
    "\n",
    "for (img, label), (img_raw, _) in zip(train_loader, train_loader):\n",
    "    img, label = img.to(DEVICE), label.to(DEVICE)\n",
    "    img.requires_grad = True\n",
    "\n",
    "    # Gradients (clean)\n",
    "    gradient_stats_batch = []\n",
    "    for i in range(img.size(0)):\n",
    "        model.zero_grad()\n",
    "        single_img = img[i:i+1].detach()\n",
    "        single_img.requires_grad = True\n",
    "        output = model(single_img)\n",
    "        target_logit = output[0, label[i].item()]\n",
    "        target_logit.backward()\n",
    "        gradients = single_img.grad.view(-1).detach().cpu().numpy()\n",
    "        gradient_stats = [np.mean(gradients), np.std(gradients), np.max(gradients), np.min(gradients)]\n",
    "        gradient_stats_batch.append(gradient_stats)\n",
    "        clean_act_norms = collect_activation_norms(model, single_img, layer_names)\n",
    "        clean_sens = [s * a for s, a in zip(sens, clean_act_norms)]\n",
    "        features_by_class[label[i].item()].append(clean_sens)\n",
    "        labels_by_class[label[i].item()].append(0)\n",
    "\n",
    "    patched = apply_patch_random_location(img, adv_patch)\n",
    "\n",
    "    gradient_stats_patch_batch = []\n",
    "    for i in range(patched.size(0)):\n",
    "        model.zero_grad()\n",
    "        single_patch = patched[i:i+1].detach()\n",
    "        single_patch.requires_grad = True\n",
    "        output_att = model(single_patch)\n",
    "        target_logit_att = output_att[0, label[i].item()]\n",
    "        target_logit_att.backward()\n",
    "        gradients_att = single_patch.grad.view(-1).detach().cpu().numpy()\n",
    "        grad_stats_att = [np.mean(gradients_att), np.std(gradients_att), np.max(gradients_att), np.min(gradients_att)]\n",
    "        gradient_stats_patch_batch.append(grad_stats_att)\n",
    "        patch_act_norms = collect_activation_norms(model, single_patch, layer_names)\n",
    "        patch_sens = [s * a for s, a in zip(sens, clean_act_norms)]\n",
    "        features_by_class[label[i].item()].append(patch_sens)\n",
    "        labels_by_class[label[i].item()].append(1)\n",
    "\n",
    "# === Train Classifiers ===\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "all_precisions, all_recalls, all_f1s = [], [], []\n",
    "\n",
    "\n",
    "classifiers = {}\n",
    "for cls in range(21):\n",
    "    X = np.array(features_by_class[cls])\n",
    "    y = np.array(labels_by_class[cls])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(f\"\\nClass {cls} detection report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Collect per-class metrics for averaging\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average='binary', pos_label=1, zero_division=0\n",
    "    )\n",
    "    all_precisions.append(precision)\n",
    "    all_recalls.append(recall)\n",
    "    all_f1s.append(f1)\n",
    "\n",
    "    diffs = np.array(iou_diffs_by_class[cls])\n",
    "    if len(diffs):\n",
    "        print(f\"IOU change for class {cls}: mean={diffs.mean():.4f}, std={diffs.std():.4f}\")\n",
    "\n",
    "    classifiers[cls] = {\n",
    "        \"classifier\": clf,\n",
    "        \"scaler\": scaler,\n",
    "    }\n",
    "\n",
    "# === Save Classifiers ===\n",
    "with open(\"figrim_res50_acts.pkl\", \"wb\") as f:\n",
    "    pickle.dump(classifiers, f)\n",
    "\n",
    "print(\"Saved per-class RF classifiers using separate sensitivities.\")\n",
    "print(\"\\n=== Average Classifier Metrics ===\")\n",
    "print(f\"Average Precision: {np.mean(all_precisions):.4f}\")\n",
    "print(f\"Average Recall:    {np.mean(all_recalls):.4f}\")\n",
    "print(f\"Average F1 Score:  {np.mean(all_f1s):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7a0dc1",
   "metadata": {},
   "source": [
    "#### Acts and Grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeb7117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "\n",
      "Class 0 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.75      0.63         8\n",
      "           1       0.50      0.29      0.36         7\n",
      "\n",
      "    accuracy                           0.53        15\n",
      "   macro avg       0.52      0.52      0.50        15\n",
      "weighted avg       0.52      0.53      0.51        15\n",
      "\n",
      "\n",
      "Class 1 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80         8\n",
      "           1       1.00      0.43      0.60         7\n",
      "\n",
      "    accuracy                           0.73        15\n",
      "   macro avg       0.83      0.71      0.70        15\n",
      "weighted avg       0.82      0.73      0.71        15\n",
      "\n",
      "\n",
      "Class 2 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.88      0.74         8\n",
      "           1       0.75      0.43      0.55         7\n",
      "\n",
      "    accuracy                           0.67        15\n",
      "   macro avg       0.69      0.65      0.64        15\n",
      "weighted avg       0.69      0.67      0.65        15\n",
      "\n",
      "\n",
      "Class 3 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.88      0.93         8\n",
      "           1       0.88      1.00      0.93         7\n",
      "\n",
      "    accuracy                           0.93        15\n",
      "   macro avg       0.94      0.94      0.93        15\n",
      "weighted avg       0.94      0.93      0.93        15\n",
      "\n",
      "\n",
      "Class 4 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96        11\n",
      "           1       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.93        15\n",
      "   macro avg       0.96      0.88      0.91        15\n",
      "weighted avg       0.94      0.93      0.93        15\n",
      "\n",
      "\n",
      "Class 5 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.93         7\n",
      "           1       1.00      0.88      0.93         8\n",
      "\n",
      "    accuracy                           0.93        15\n",
      "   macro avg       0.94      0.94      0.93        15\n",
      "weighted avg       0.94      0.93      0.93        15\n",
      "\n",
      "\n",
      "Class 6 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.78      0.82         9\n",
      "           1       0.71      0.83      0.77         6\n",
      "\n",
      "    accuracy                           0.80        15\n",
      "   macro avg       0.79      0.81      0.80        15\n",
      "weighted avg       0.81      0.80      0.80        15\n",
      "\n",
      "\n",
      "Class 7 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      1.00      0.84         8\n",
      "           1       1.00      0.57      0.73         7\n",
      "\n",
      "    accuracy                           0.80        15\n",
      "   macro avg       0.86      0.79      0.78        15\n",
      "weighted avg       0.85      0.80      0.79        15\n",
      "\n",
      "\n",
      "Class 8 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.80      0.62         5\n",
      "           1       0.86      0.60      0.71        10\n",
      "\n",
      "    accuracy                           0.67        15\n",
      "   macro avg       0.68      0.70      0.66        15\n",
      "weighted avg       0.74      0.67      0.68        15\n",
      "\n",
      "\n",
      "Class 9 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95        10\n",
      "           1       1.00      0.80      0.89         5\n",
      "\n",
      "    accuracy                           0.93        15\n",
      "   macro avg       0.95      0.90      0.92        15\n",
      "weighted avg       0.94      0.93      0.93        15\n",
      "\n",
      "\n",
      "Class 10 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      1.00      0.60         6\n",
      "           1       1.00      0.11      0.20         9\n",
      "\n",
      "    accuracy                           0.47        15\n",
      "   macro avg       0.71      0.56      0.40        15\n",
      "weighted avg       0.77      0.47      0.36        15\n",
      "\n",
      "\n",
      "Class 11 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.83      0.71         6\n",
      "           1       0.88      0.70      0.78        10\n",
      "\n",
      "    accuracy                           0.75        16\n",
      "   macro avg       0.75      0.77      0.75        16\n",
      "weighted avg       0.78      0.75      0.75        16\n",
      "\n",
      "\n",
      "Class 12 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         8\n",
      "           1       1.00      0.71      0.83         7\n",
      "\n",
      "    accuracy                           0.87        15\n",
      "   macro avg       0.90      0.86      0.86        15\n",
      "weighted avg       0.89      0.87      0.86        15\n",
      "\n",
      "\n",
      "Class 13 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      1.00      0.90         9\n",
      "           1       1.00      0.67      0.80         6\n",
      "\n",
      "    accuracy                           0.87        15\n",
      "   macro avg       0.91      0.83      0.85        15\n",
      "weighted avg       0.89      0.87      0.86        15\n",
      "\n",
      "\n",
      "Class 14 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      1.00      0.77         5\n",
      "           1       1.00      0.70      0.82        10\n",
      "\n",
      "    accuracy                           0.80        15\n",
      "   macro avg       0.81      0.85      0.80        15\n",
      "weighted avg       0.88      0.80      0.81        15\n",
      "\n",
      "\n",
      "Class 15 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.83      0.67         6\n",
      "           1       0.83      0.56      0.67         9\n",
      "\n",
      "    accuracy                           0.67        15\n",
      "   macro avg       0.69      0.69      0.67        15\n",
      "weighted avg       0.72      0.67      0.67        15\n",
      "\n",
      "\n",
      "Class 16 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90        10\n",
      "           1       0.80      0.80      0.80         5\n",
      "\n",
      "    accuracy                           0.87        15\n",
      "   macro avg       0.85      0.85      0.85        15\n",
      "weighted avg       0.87      0.87      0.87        15\n",
      "\n",
      "\n",
      "Class 17 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.90      0.86        10\n",
      "           1       0.75      0.60      0.67         5\n",
      "\n",
      "    accuracy                           0.80        15\n",
      "   macro avg       0.78      0.75      0.76        15\n",
      "weighted avg       0.80      0.80      0.79        15\n",
      "\n",
      "\n",
      "Class 18 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86         6\n",
      "           1       1.00      0.78      0.88         9\n",
      "\n",
      "    accuracy                           0.87        15\n",
      "   macro avg       0.88      0.89      0.87        15\n",
      "weighted avg       0.90      0.87      0.87        15\n",
      "\n",
      "\n",
      "Class 19 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.83      0.77         6\n",
      "           1       0.88      0.78      0.82         9\n",
      "\n",
      "    accuracy                           0.80        15\n",
      "   macro avg       0.79      0.81      0.80        15\n",
      "weighted avg       0.81      0.80      0.80        15\n",
      "\n",
      "\n",
      "Class 20 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.71      0.56         7\n",
      "           1       0.50      0.25      0.33         8\n",
      "\n",
      "    accuracy                           0.47        15\n",
      "   macro avg       0.48      0.48      0.44        15\n",
      "weighted avg       0.48      0.47      0.44        15\n",
      "\n",
      "Saved per-class RF classifiers using separate sensitivities.\n",
      "\n",
      "=== Average Classifier Metrics ===\n",
      "Average Precision: 0.8728\n",
      "Average Recall:    0.6298\n",
      "Average F1 Score:  0.7107\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def apply_patch_random_location(img, adv_patch):\n",
    "    \"\"\"\n",
    "    Apply adversarial patch at a random location for each image in the batch.\n",
    "    Args:\n",
    "        img: Tensor of shape [B, C, H, W]\n",
    "        adv_patch: Tensor of shape [C, ph, pw]\n",
    "    Returns:\n",
    "        patched: Tensor of shape [B, C, H, W] with the patch applied\n",
    "    \"\"\"\n",
    "    patched = img.clone()\n",
    "    B, C, H, W = img.shape\n",
    "    ph, pw = adv_patch.shape[1:]\n",
    "\n",
    "    for i in range(B):\n",
    "        x = torch.randint(0, H - ph + 1, (1,)).item()\n",
    "        y = torch.randint(0, W - pw + 1, (1,)).item()\n",
    "        patched[i, :, x:x+ph, y:y+pw] = adv_patch  # Apply patch to each image\n",
    "\n",
    "    return patched\n",
    "\n",
    "\n",
    "unique_labels = set()\n",
    "for _, lbl in train_loader:\n",
    "    unique_labels.update(lbl.tolist())\n",
    "features_by_class = {int(k): [] for k in sorted(unique_labels)}\n",
    "labels_by_class   = {int(k): [] for k in sorted(unique_labels)}\n",
    "iou_diffs_by_class= {int(k): [] for k in sorted(unique_labels)}\n",
    "\n",
    "\n",
    "all_labels = []\n",
    "for _, lbl in train_loader:\n",
    "    all_labels.extend(lbl.tolist())\n",
    "print(\"Unique labels:\", sorted(set(all_labels))[:30])\n",
    "\n",
    "\n",
    "for (img, label), (img_raw, _) in zip(train_loader, train_loader):\n",
    "    img, label = img.to(DEVICE), label.to(DEVICE)\n",
    "    img.requires_grad = True\n",
    "\n",
    "    # Clean sensitivities\n",
    "\n",
    "    # Gradients (clean)\n",
    "    gradient_stats_batch = []\n",
    "    for i in range(img.size(0)):\n",
    "        model.zero_grad()\n",
    "        single_img = img[i:i+1].detach()\n",
    "        single_img.requires_grad = True\n",
    "        output = model(single_img)\n",
    "        target_logit = output[0, label[i].item()]\n",
    "        target_logit.backward()\n",
    "        gradients = single_img.grad.view(-1).detach().cpu().numpy()\n",
    "        gradient_stats = [np.mean(gradients), np.std(gradients), np.max(gradients), np.min(gradients)]\n",
    "        gradient_stats_batch.append(gradient_stats)\n",
    "        clean_act_norms = collect_activation_norms(model, single_img, layer_names)\n",
    "        clean_sens = [s * a for s, a in zip(sens, clean_act_norms)]\n",
    "        feat_vec = np.concatenate([\n",
    "            0.85 * np.array(patch_sens, dtype=float),\n",
    "            0.15  * np.array(grad_stats_att, dtype=float),\n",
    "        ])\n",
    "\n",
    "        features_by_class[label[i].item()].append(feat_vec.tolist())\n",
    "        #features_by_class[label[i].item()].append(clean_sens + gradient_stats)\n",
    "        labels_by_class[label[i].item()].append(0)\n",
    "\n",
    "\n",
    "    patched = apply_patch_random_location(img, adv_patch)\n",
    "\n",
    "    gradient_stats_patch_batch = []\n",
    "    for i in range(patched.size(0)):\n",
    "        model.zero_grad()\n",
    "        single_patch = patched[i:i+1].detach()\n",
    "        single_patch.requires_grad = True\n",
    "        output_att = model(single_patch)\n",
    "        target_logit_att = output_att[0, label[i].item()]\n",
    "        target_logit_att.backward()\n",
    "        gradients_att = single_patch.grad.view(-1).detach().cpu().numpy()\n",
    "        grad_stats_att = [np.mean(gradients_att), np.std(gradients_att), np.max(gradients_att), np.min(gradients_att)]\n",
    "        gradient_stats_patch_batch.append(grad_stats_att)\n",
    "        patch_act_norms = collect_activation_norms(model, single_patch, layer_names)\n",
    "        patch_sens = [s * a for s, a in zip(sens, clean_act_norms)]\n",
    "        feat_vec = np.concatenate([\n",
    "            0.85 * np.array(patch_sens, dtype=float),\n",
    "            0.15  * np.array(grad_stats_att, dtype=float),\n",
    "            #0.05 * np.array([iou_diff], dtype=float)\n",
    "        ])\n",
    "\n",
    "        features_by_class[label[i].item()].append(feat_vec.tolist())\n",
    "        #features_by_class[label[i].item()].append(patch_sens + grad_stats_att)\n",
    "        labels_by_class[label[i].item()].append(1)\n",
    "\n",
    "# === Train Classifiers ===\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "all_precisions, all_recalls, all_f1s = [], [], []\n",
    "\n",
    "\n",
    "classifiers = {}\n",
    "for cls in range(21):\n",
    "    X = np.array(features_by_class[cls])\n",
    "    y = np.array(labels_by_class[cls])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(f\"\\nClass {cls} detection report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Collect per-class metrics for averaging\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average='binary', pos_label=1, zero_division=0\n",
    "    )\n",
    "    all_precisions.append(precision)\n",
    "    all_recalls.append(recall)\n",
    "    all_f1s.append(f1)\n",
    "\n",
    "    diffs = np.array(iou_diffs_by_class[cls])\n",
    "    if len(diffs):\n",
    "        print(f\"IOU change for class {cls}: mean={diffs.mean():.4f}, std={diffs.std():.4f}\")\n",
    "\n",
    "    classifiers[cls] = {\n",
    "        \"classifier\": clf,\n",
    "        \"scaler\": scaler,\n",
    "    } \n",
    "\n",
    "# === Save Classifiers ===\n",
    "with open(\"figrim_res50_acts_grads.pkl\", \"wb\") as f:\n",
    "    pickle.dump(classifiers, f)\n",
    "\n",
    "print(\"Saved per-class RF classifiers using separate sensitivities.\")\n",
    "print(\"\\n=== Average Classifier Metrics ===\")\n",
    "print(f\"Average Precision: {np.mean(all_precisions):.4f}\")\n",
    "print(f\"Average Recall:    {np.mean(all_recalls):.4f}\")\n",
    "print(f\"Average F1 Score:  {np.mean(all_f1s):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2ad31b",
   "metadata": {},
   "source": [
    "#### Acts, grads and saliency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a5fb535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "\n",
      "Class 0 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         6\n",
      "           1       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           1.00        15\n",
      "   macro avg       1.00      1.00      1.00        15\n",
      "weighted avg       1.00      1.00      1.00        15\n",
      "\n",
      "\n",
      "Class 1 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67         8\n",
      "           1       0.64      1.00      0.78         7\n",
      "\n",
      "    accuracy                           0.73        15\n",
      "   macro avg       0.82      0.75      0.72        15\n",
      "weighted avg       0.83      0.73      0.72        15\n",
      "\n",
      "\n",
      "Class 2 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      1.00      0.71         5\n",
      "           1       1.00      0.60      0.75        10\n",
      "\n",
      "    accuracy                           0.73        15\n",
      "   macro avg       0.78      0.80      0.73        15\n",
      "weighted avg       0.85      0.73      0.74        15\n",
      "\n",
      "\n",
      "Class 3 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94         9\n",
      "           1       0.86      1.00      0.92         6\n",
      "\n",
      "    accuracy                           0.93        15\n",
      "   macro avg       0.93      0.94      0.93        15\n",
      "weighted avg       0.94      0.93      0.93        15\n",
      "\n",
      "\n",
      "Class 4 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.80      0.62         5\n",
      "           1       0.86      0.60      0.71        10\n",
      "\n",
      "    accuracy                           0.67        15\n",
      "   macro avg       0.68      0.70      0.66        15\n",
      "weighted avg       0.74      0.67      0.68        15\n",
      "\n",
      "\n",
      "Class 5 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.71      0.71         7\n",
      "           1       0.75      0.75      0.75         8\n",
      "\n",
      "    accuracy                           0.73        15\n",
      "   macro avg       0.73      0.73      0.73        15\n",
      "weighted avg       0.73      0.73      0.73        15\n",
      "\n",
      "\n",
      "Class 6 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.83      0.91         6\n",
      "           1       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.93        15\n",
      "   macro avg       0.95      0.92      0.93        15\n",
      "weighted avg       0.94      0.93      0.93        15\n",
      "\n",
      "\n",
      "Class 7 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.75      0.80         8\n",
      "           1       0.75      0.86      0.80         7\n",
      "\n",
      "    accuracy                           0.80        15\n",
      "   macro avg       0.80      0.80      0.80        15\n",
      "weighted avg       0.81      0.80      0.80        15\n",
      "\n",
      "\n",
      "Class 8 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      1.00      0.77         5\n",
      "           1       1.00      0.70      0.82        10\n",
      "\n",
      "    accuracy                           0.80        15\n",
      "   macro avg       0.81      0.85      0.80        15\n",
      "weighted avg       0.88      0.80      0.81        15\n",
      "\n",
      "\n",
      "Class 9 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.93         7\n",
      "           1       1.00      0.88      0.93         8\n",
      "\n",
      "    accuracy                           0.93        15\n",
      "   macro avg       0.94      0.94      0.93        15\n",
      "weighted avg       0.94      0.93      0.93        15\n",
      "\n",
      "\n",
      "Class 10 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.62      0.77         8\n",
      "           1       0.70      1.00      0.82         7\n",
      "\n",
      "    accuracy                           0.80        15\n",
      "   macro avg       0.85      0.81      0.80        15\n",
      "weighted avg       0.86      0.80      0.79        15\n",
      "\n",
      "\n",
      "Class 11 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.29      0.33         7\n",
      "           1       0.55      0.67      0.60         9\n",
      "\n",
      "    accuracy                           0.50        16\n",
      "   macro avg       0.47      0.48      0.47        16\n",
      "weighted avg       0.48      0.50      0.48        16\n",
      "\n",
      "\n",
      "Class 12 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80         6\n",
      "           1       0.82      1.00      0.90         9\n",
      "\n",
      "    accuracy                           0.87        15\n",
      "   macro avg       0.91      0.83      0.85        15\n",
      "weighted avg       0.89      0.87      0.86        15\n",
      "\n",
      "\n",
      "Class 13 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.80      0.62         5\n",
      "           1       0.86      0.60      0.71        10\n",
      "\n",
      "    accuracy                           0.67        15\n",
      "   macro avg       0.68      0.70      0.66        15\n",
      "weighted avg       0.74      0.67      0.68        15\n",
      "\n",
      "\n",
      "Class 14 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.43      0.55         7\n",
      "           1       0.64      0.88      0.74         8\n",
      "\n",
      "    accuracy                           0.67        15\n",
      "   macro avg       0.69      0.65      0.64        15\n",
      "weighted avg       0.69      0.67      0.65        15\n",
      "\n",
      "\n",
      "Class 15 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.75      0.86         8\n",
      "           1       0.78      1.00      0.88         7\n",
      "\n",
      "    accuracy                           0.87        15\n",
      "   macro avg       0.89      0.88      0.87        15\n",
      "weighted avg       0.90      0.87      0.87        15\n",
      "\n",
      "\n",
      "Class 16 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.62      0.77         8\n",
      "           1       0.70      1.00      0.82         7\n",
      "\n",
      "    accuracy                           0.80        15\n",
      "   macro avg       0.85      0.81      0.80        15\n",
      "weighted avg       0.86      0.80      0.79        15\n",
      "\n",
      "\n",
      "Class 17 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.71      0.83         7\n",
      "           1       0.80      1.00      0.89         8\n",
      "\n",
      "    accuracy                           0.87        15\n",
      "   macro avg       0.90      0.86      0.86        15\n",
      "weighted avg       0.89      0.87      0.86        15\n",
      "\n",
      "\n",
      "Class 18 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.83      0.91         6\n",
      "           1       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.93        15\n",
      "   macro avg       0.95      0.92      0.93        15\n",
      "weighted avg       0.94      0.93      0.93        15\n",
      "\n",
      "\n",
      "Class 19 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.70      0.82        10\n",
      "           1       0.62      1.00      0.77         5\n",
      "\n",
      "    accuracy                           0.80        15\n",
      "   macro avg       0.81      0.85      0.80        15\n",
      "weighted avg       0.88      0.80      0.81        15\n",
      "\n",
      "\n",
      "Class 20 detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      1.00      0.90         9\n",
      "           1       1.00      0.67      0.80         6\n",
      "\n",
      "    accuracy                           0.87        15\n",
      "   macro avg       0.91      0.83      0.85        15\n",
      "weighted avg       0.89      0.87      0.86        15\n",
      "\n",
      "Saved per-class RF classifiers using separate sensitivities.\n",
      "\n",
      "=== Average Classifier Metrics ===\n",
      "Average Precision: 0.8148\n",
      "Average Recall:    0.8662\n",
      "Average F1 Score:  0.8229\n"
     ]
    }
   ],
   "source": [
    "\n",
    "unique_labels = set()\n",
    "for _, lbl in train_loader:\n",
    "    unique_labels.update(lbl.tolist())\n",
    "features_by_class = {int(k): [] for k in sorted(unique_labels)}\n",
    "labels_by_class   = {int(k): [] for k in sorted(unique_labels)}\n",
    "iou_diffs_by_class= {int(k): [] for k in sorted(unique_labels)}\n",
    "\n",
    "sal_model = sal_model.to(device)\n",
    "\n",
    "all_labels = []\n",
    "for _, lbl in train_loader:\n",
    "    all_labels.extend(lbl.tolist())\n",
    "print(\"Unique labels:\", sorted(set(all_labels))[:30])\n",
    "\n",
    "\n",
    "for (img, label), (img_raw, _) in zip(train_loader, train_loader):\n",
    "    img, label = img.to(DEVICE), label.to(DEVICE)\n",
    "    img.requires_grad = True\n",
    "\n",
    "    # Clean sensitivities\n",
    "    target_layer = [m for n, m in model.named_modules() if isinstance(m, torch.nn.Conv2d)][-1]\n",
    "    cam = GradCAM(model=model, target_layers=[target_layer])\n",
    "\n",
    "    # Gradients (clean)\n",
    "    gradient_stats_batch = []\n",
    "    for i in range(img.size(0)):\n",
    "        model.zero_grad()\n",
    "        single_img = img[i:i+1].detach()\n",
    "        single_img.requires_grad = True\n",
    "        output = model(single_img)\n",
    "        target_logit = output[0, label[i].item()]\n",
    "        target_logit.backward()\n",
    "        gradients = single_img.grad.view(-1).detach().cpu().numpy()\n",
    "        gradient_stats = [np.mean(gradients), np.std(gradients), np.max(gradients), np.min(gradients)]\n",
    "        gradient_stats_batch.append(gradient_stats)\n",
    "        clean_act_norms = collect_activation_norms(model, single_img, layer_names)\n",
    "        clean_sens = [s * a for s, a in zip(sens, clean_act_norms)]\n",
    "\n",
    "        if single_img.ndim == 3:\n",
    "            single_img = single_img.unsqueeze(0) \n",
    "\n",
    "        cam_map = cam(input_tensor=single_img, targets=[ClassifierOutputTarget(output.argmax().item()+1)])[0]\n",
    "        cam_min, cam_max = cam_map.min(), cam_map.max()\n",
    "        cam_norm = (cam_map - cam_min) / (cam_max - cam_min + 1e-8)\n",
    "        th1 = np.quantile(cam_norm, 0.80)\n",
    "        gradcam_mask = (cam_norm >= th1).astype(np.uint8)\n",
    "        \n",
    "        sal_out = torch.sigmoid(sal_model(single_img)['out']).squeeze().detach().cpu().numpy()\n",
    "        sal_min, sal_max = sal_out.min(), sal_out.max()\n",
    "        sal_norm = (sal_out - sal_min) / (sal_max - sal_min + 1e-8)\n",
    "        th2 = np.quantile(sal_norm, 0.80)\n",
    "        human_mask = (sal_norm >= th2).astype(np.uint8)\n",
    "\n",
    "        if gradcam_mask.shape != human_mask.shape:\n",
    "            gradcam_mask = cv2.resize(gradcam_mask.astype(np.float32), (human_mask.shape[1], human_mask.shape[0]))\n",
    "            gradcam_mask = (gradcam_mask > 1).astype(np.uint8)\n",
    "\n",
    "        iou_diff = 1 - compute_iou(gradcam_mask, human_mask)\n",
    "        #print(clean_sens, gradient_stats, iou_diff)\n",
    "        feat_vec =  np.concatenate([\n",
    "            .20 * np.array(clean_sens, dtype=float),\n",
    "            .70  * np.array(gradient_stats, dtype=float),\n",
    "            .05 * np.array([iou_diff], dtype=float)\n",
    "        ])\n",
    "        features_by_class[label[i].item()].append(feat_vec.tolist())\n",
    "        labels_by_class[label[i].item()].append(0)\n",
    "\n",
    "\n",
    "    patched = apply_patch_random_location(img, adv_patch)\n",
    "\n",
    "    gradient_stats_patch_batch = []\n",
    "    for i in range(patched.size(0)):\n",
    "        model.zero_grad()\n",
    "        single_patch = patched[i:i+1].detach()\n",
    "        single_patch.requires_grad = True\n",
    "        output_att = model(single_patch)\n",
    "        target_logit_att = output_att[0, label[i].item()]\n",
    "        target_logit_att.backward()\n",
    "        gradients_att = single_patch.grad.view(-1).detach().cpu().numpy()\n",
    "        grad_stats_att = [np.mean(gradients_att), np.std(gradients_att), np.max(gradients_att), np.min(gradients_att)]\n",
    "        gradient_stats_patch_batch.append(grad_stats_att)\n",
    "        patch_act_norms = collect_activation_norms(model, single_patch, layer_names)\n",
    "        patch_sens = [s * a for s, a in zip(sens, clean_act_norms)]\n",
    "\n",
    "\n",
    "        if single_patch.ndim == 3:\n",
    "            single_patch = single_patch.unsqueeze(0)\n",
    "\n",
    "        cam_map = cam(input_tensor=single_patch, targets=[ClassifierOutputTarget(output.argmax().item()+1)])[0]\n",
    "        cam_min, cam_max = cam_map.min(), cam_map.max()\n",
    "        cam_norm = (cam_map - cam_min) / (cam_max - cam_min + 1e-8)\n",
    "        th1 = np.quantile(cam_norm, 0.80)\n",
    "        gradcam_mask = (cam_norm >= th1).astype(np.uint8)\n",
    "        \n",
    "        sal_out = torch.sigmoid(sal_model(single_img)['out']).squeeze().detach().cpu().numpy()\n",
    "        sal_min, sal_max = sal_out.min(), sal_out.max()\n",
    "        sal_norm = (sal_out - sal_min) / (sal_max - sal_min + 1e-8)\n",
    "        th2 = np.quantile(sal_norm, 0.80)\n",
    "        human_mask = (sal_norm >= th2).astype(np.uint8)\n",
    "\n",
    "        if gradcam_mask.shape != human_mask.shape:\n",
    "            gradcam_mask = cv2.resize(gradcam_mask.astype(np.float32), (human_mask.shape[1], human_mask.shape[0]))\n",
    "            gradcam_mask = (gradcam_mask > 1).astype(np.uint8)\n",
    "        \n",
    "        iou_diff = 1 - compute_iou(gradcam_mask, human_mask)\n",
    "\n",
    "        feat_vec = np.concatenate([\n",
    "            .20 * np.array(patch_sens, dtype=float),\n",
    "            .70  * np.array(grad_stats_att, dtype=float),\n",
    "            .05 * np.array([iou_diff], dtype=float)\n",
    "        ])\n",
    "\n",
    "        features_by_class[label[i].item()].append(feat_vec.tolist())\n",
    "        labels_by_class[label[i].item()].append(1)\n",
    "\n",
    "# === Train Classifiers ===\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "all_precisions, all_recalls, all_f1s = [], [], []\n",
    "\n",
    "classifiers = {}\n",
    "for cls in range(21):\n",
    "    X = np.array(features_by_class[cls])\n",
    "    y = np.array(labels_by_class[cls])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(f\"\\nClass {cls} detection report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Collect per-class metrics for averaging\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average='binary', pos_label=1, zero_division=0\n",
    "    )\n",
    "    all_precisions.append(precision)\n",
    "    all_recalls.append(recall)\n",
    "    all_f1s.append(f1)\n",
    "\n",
    "    diffs = np.array(iou_diffs_by_class[cls])\n",
    "    if len(diffs):\n",
    "        print(f\"IOU change for class {cls}: mean={diffs.mean():.4f}, std={diffs.std():.4f}\")\n",
    "\n",
    "    classifiers[cls] = {\n",
    "        \"classifier\": clf,\n",
    "        \"scaler\": scaler,\n",
    "    }\n",
    "\n",
    "# === Save Classifiers ===\n",
    "with open(\"figrim_res50_acts_grads_iou.pkl\", \"wb\") as f:\n",
    "    pickle.dump(classifiers, f)\n",
    "\n",
    "print(\"Saved per-class RF classifiers using separate sensitivities.\")\n",
    "print(\"\\n=== Average Classifier Metrics ===\")\n",
    "print(f\"Average Precision: {np.mean(all_precisions):.4f}\")\n",
    "print(f\"Average Recall:    {np.mean(all_recalls):.4f}\")\n",
    "print(f\"Average F1 Score:  {np.mean(all_f1s):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
