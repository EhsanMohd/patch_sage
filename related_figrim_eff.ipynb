{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e238f100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load \n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse, time, os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Load \n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse, time, os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "\n",
    "# Define preprocessing transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to model input size\n",
    "    transforms.ToTensor(),          # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define the dataset\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "seed = 42\n",
    "root = \"/home/mrahm326/experiments/Probing Mechanism/figrim/Targets\"\n",
    "\n",
    "# Your existing train transform (from your snippet)\n",
    "train_transform = transform\n",
    "\n",
    "# Simple eval transform (adjust to your preprocessing: size/normalize, etc.)\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load once without transforms to read labels for a stratified split\n",
    "full_base = datasets.ImageFolder(root=root)\n",
    "y = np.array(full_base.targets)\n",
    "idx = np.arange(len(y))\n",
    "\n",
    "# Stratified split (keeps class balance)\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n",
    "train_idx, test_idx = next(sss.split(idx, y))\n",
    "\n",
    "# Build separate datasets so train/test can have different transforms\n",
    "train_dataset = Subset(datasets.ImageFolder(root=root, transform=train_transform), train_idx.tolist())\n",
    "test_dataset  = Subset(datasets.ImageFolder(root=root, transform=eval_transform),  test_idx.tolist())\n",
    "\n",
    "# (Optional) DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True,  num_workers=4, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import efficientnet_b0\n",
    "from torchvision import transforms\n",
    "\n",
    "num_classes = len(full_base.classes)\n",
    "print(f\"Found {num_classes} classes · {len(train_dataset)} train images · {len(test_dataset)} val images\")\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "# Set device first\n",
    "device = \"cuda:1\"\n",
    "\n",
    "# Assume adv_patch is a tensor\n",
    "def create_patch(patch_size=0.2, image_size=224):\n",
    "    patch_dim = int(patch_size * image_size)\n",
    "    patch = torch.rand(3, patch_dim, patch_dim, requires_grad=True, device=device)\n",
    "    return patch\n",
    "#torch.save(adv_patch, 'adversarial_patch.pth')\n",
    "adv_patch = create_patch(patch_size=0.2, image_size=224)\n",
    "import torch\n",
    "\n",
    "# Load the saved patch\n",
    "adv_patch = torch.load('/home/mrahm326/experiments/Probing Mechanism/DeepGaze/Figrim_ALL/adversarial_patch_figrim_eff.pth', map_location=device)\n",
    "\n",
    "# If you need it to require gradients for optimization or testing:\n",
    "adv_patch.requires_grad = True\n",
    "\n",
    "# Confirm shape and device\n",
    "print(f\"Loaded patch shape: {adv_patch.shape}, requires_grad: {adv_patch.requires_grad}, device: {adv_patch.device}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sal_model = models.segmentation.deeplabv3_resnet50(weights=None, num_classes=1)  # Single channel output\n",
    "\n",
    "from torchvision import models\n",
    "\n",
    "# Load pretrained model\n",
    "sal_model = models.segmentation.deeplabv3_resnet50(weights=models.segmentation.DeepLabV3_ResNet50_Weights.DEFAULT)\n",
    "\n",
    "# Replace the classifier to output a single-channel saliency map\n",
    "sal_model.classifier[-1] = torch.nn.Conv2d(256, 1, kernel_size=1)\n",
    "# Load the best trained model\n",
    "sal_model.load_state_dict(torch.load(\"/home/mrahm326/experiments/Probing Mechanism/DeepGaze/Figrim_ALL/best_deeplabv3_saliency_figrim.pt\"))\n",
    "sal_model.eval()\n",
    "\n",
    "# Pretrained EfficientNet-B0\n",
    "model_eff = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Replace the classifier head for your number of classes\n",
    "in_features = model_eff.classifier[1].in_features  # linear layer is at index 1\n",
    "model_eff.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "model_eff = model_eff.to(device)\n",
    "model = model_eff \n",
    "\n",
    "model.load_state_dict(torch.load(\"best_eff_figrim.pt\"))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e23898a",
   "metadata": {},
   "source": [
    "### Dataset for Adversarial Patches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd71bda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "DEVICE = \"cuda:1\"\n",
    "\n",
    "# ==== Patch Application Function ====\n",
    "def apply_patch(image_tensor, patch_tensor, device):\n",
    "    patch_tensor = patch_tensor.to(device)\n",
    "    patched_image = image_tensor.clone()\n",
    "    _, H, W = image_tensor.shape\n",
    "    ph, pw = patch_tensor.shape[1:]\n",
    "    top = torch.randint(0, H - ph + 1, (1,)).item()\n",
    "    left = torch.randint(0, W - pw + 1, (1,)).item()\n",
    "    patched_image[:, top:top+ph, left:left+pw] = patch_tensor\n",
    "    return patched_image\n",
    "\n",
    "def apply_patch(image_tensor, patch_tensor, device):\n",
    "    patch_tensor = patch_tensor.to(device)\n",
    "    if patch_tensor.dim() == 4:  # remove batch dimension if present\n",
    "        patch_tensor = patch_tensor.squeeze(0)\n",
    "    patched_image = image_tensor.clone()\n",
    "    _, H, W = image_tensor.shape\n",
    "    _, ph, pw = patch_tensor.shape\n",
    "    top = torch.randint(0, H - ph + 1, (1,)).item()\n",
    "    left = torch.randint(0, W - pw + 1, (1,)).item()\n",
    "    patched_image[:, top:top+ph, left:left+pw] = patch_tensor\n",
    "    return patched_image\n",
    "\n",
    "\n",
    "# ==== Custom Dataset with dual labels ====\n",
    "class PatchDetectionDataset(Dataset):\n",
    "    def __init__(self, root_dir, patch_tensor, transform=None, device=\"cpu\"):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.patch = patch_tensor\n",
    "        self.device = device\n",
    "\n",
    "        self.image_paths = []   # (path, class_label, adv_flag)\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(sorted(os.listdir(root_dir)))}\n",
    "\n",
    "        for class_name, class_idx in self.class_to_idx.items():\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            if not os.path.isdir(class_path):\n",
    "                continue\n",
    "\n",
    "            for img_name in os.listdir(class_path):\n",
    "                img_path = os.path.join(class_path, img_name)\n",
    "\n",
    "                if os.path.isdir(img_path):\n",
    "                    continue\n",
    "                if not img_name.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tif')):\n",
    "                    continue\n",
    "\n",
    "                # Add both clean and adversarial versions\n",
    "                self.image_paths.append((img_path, class_idx, 0))  # clean\n",
    "                self.image_paths.append((img_path, class_idx, 1))  # patched\n",
    "\n",
    "        # Expose labels for stratified splitting\n",
    "        self.targets = [adv_flag for _, _, adv_flag in self.image_paths]\n",
    "        self.class_labels = [class_idx for _, class_idx, _ in self.image_paths]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, class_label, adv_flag = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image_tensor = self.transform(image) if self.transform else transforms.ToTensor()(image)\n",
    "\n",
    "        if adv_flag == 1:\n",
    "            image_tensor = apply_patch(image_tensor, self.patch, self.device)\n",
    "\n",
    "        # Return both labels\n",
    "        return image_tensor, class_label, adv_flag\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = PatchDetectionDataset(\n",
    "    root_dir=\"/home/mrahm326/experiments/Probing Mechanism/figrim/Targets\",\n",
    "    patch_tensor=adv_patch,\n",
    "    transform=transform,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# Stratify by adv_flag (clean vs patched)\n",
    "y = np.array(train_dataset.targets)\n",
    "idx = np.arange(len(y))\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(sss.split(idx, y))\n",
    "\n",
    "train_dataset_split = Subset(train_dataset, train_idx.tolist())\n",
    "test_dataset_split  = Subset(train_dataset, test_idx.tolist())\n",
    "\n",
    "train_loader = DataLoader(train_dataset_split, batch_size=64, shuffle=True, num_workers=0, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset_split,  batch_size=16, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# Example batch\n",
    "images, class_labels, adv_flags = next(iter(train_loader))\n",
    "print(images.shape, class_labels[:5], adv_flags[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d4d600",
   "metadata": {},
   "source": [
    "### Probing Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aa0147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ===== DETECTOR MODULE =====\n",
    "class ProbingDetector(nn.Module):\n",
    "    def __init__(self, base_model, layer_names, feature_dim=64):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        self.layer_names = layer_names\n",
    "        self.features = {}\n",
    "        self.probes = nn.ModuleList()\n",
    "\n",
    "        # Hook & MLP for each layer\n",
    "        for name in layer_names:\n",
    "            layer = dict([*self.base.named_modules()])[name]\n",
    "\n",
    "            def get_hook(n):\n",
    "                def hook(module, input, output):\n",
    "                    self.features[n] = output.detach()\n",
    "                return hook\n",
    "            layer.register_forward_hook(get_hook(name))\n",
    "\n",
    "            # Determine number of channels\n",
    "            out_channels = layer.out_channels if hasattr(layer, 'out_channels') else 64\n",
    "            self.probes.append(nn.Sequential(\n",
    "                nn.AdaptiveAvgPool2d(1),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(out_channels, feature_dim),\n",
    "                nn.ReLU()\n",
    "            ))\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(len(layer_names) * feature_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.features = {name: None for name in self.layer_names}\n",
    "        _ = self.base(x)\n",
    "\n",
    "        processed = []\n",
    "        for i, name in enumerate(self.layer_names):\n",
    "            feat = self.features[name]\n",
    "            if feat is not None:\n",
    "                processed_feat = self.probes[i](feat)\n",
    "                processed.append(processed_feat)\n",
    "\n",
    "        concat = torch.cat(processed, dim=1)\n",
    "        return self.classifier(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41985971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ===== Training Configuration =====\n",
    "EPOCHS = 10\n",
    "LR = 1e-3\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "LAYER_NAMES = ['']  # For ResNet\n",
    "LAYER_NAMES = [\n",
    "    \"layer1.0.conv1\",   # early\n",
    "    \"layer2.0.conv1\",   # mid\n",
    "    \"layer3.0.conv1\",   # deeper\n",
    "    \"layer4.0.conv1\"    # deepest before avgpool\n",
    "]\n",
    "\n",
    "def list_conv2d_names(m):\n",
    "    return [name for name, mod in m.named_modules() if isinstance(mod, nn.Conv2d)]\n",
    "\n",
    "conv_names = list_conv2d_names(model)\n",
    "print(conv_names[:10], \"... total:\", len(conv_names))\n",
    "\n",
    "LAYER_NAMES = [ # For EffNet\n",
    "    \"features.0.0\",                # ✅ Early stem conv (3 → 32 channels, stride=2)\n",
    "    \"features.3.0.block.1.0\",      # ✅ Middle depthwise conv (5×5, stride=2, ~24→40 channels)\n",
    "    \"features.5.1.block.1.0\",      # ✅ Deeper depthwise conv (5×5, ~112→672 channels)\n",
    "    \"features.8.0\"                 # ✅ Deepest 1×1 conv before classifier (320 → 1280)\n",
    "]\n",
    "\n",
    "\n",
    "IMAGE_SIZE = (224, 224)\n",
    "FEATURE_DIM = 64  # Size of each layer’s embedding\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "detector = ProbingDetector(model, LAYER_NAMES, feature_dim=FEATURE_DIM).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(detector.parameters(), lr=LR)\n",
    "\n",
    "detector.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = correct = total = 0\n",
    "\n",
    "    for images, class_labels, adv_flags in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        images = images.to(DEVICE)\n",
    "        labels = adv_flags.to(DEVICE).long()      # 0=clean, 1=patched\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = detector(images)                # logits shape [B, 2]\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    accuracy = 100.0 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {epoch_loss:.4f} - Acc: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6f591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ===== Assume you already have these =====\n",
    "# - test_loader\n",
    "# - base_model (your pretrained classifier)\n",
    "# - LAYER_NAMES (list of layer names to hook)\n",
    "# - FEATURE_DIM (dimensionality of each probe head)\n",
    "\n",
    "# Wrap your base model with the probing detector\n",
    "\n",
    "detector.eval()\n",
    "\n",
    "# ===== Inference and Evaluation =====\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels, adv_flags in test_loader:\n",
    "        images = images.to(DEVICE)\n",
    "        labels = adv_flags.to(DEVICE)\n",
    "        \n",
    "        outputs = detector(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "# ===== Report =====\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=[\"Benign\", \"Adversarial\"]))\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Per-class (0=Benign, 1=Adversarial)\n",
    "p_cls, r_cls, f1_cls, sup_cls = precision_recall_fscore_support(\n",
    "    all_labels, all_preds, labels=[0, 1], average=None, zero_division=0\n",
    ")\n",
    "print(f\"Benign (0):       Precision={p_cls[0]:.4f}  Recall={r_cls[0]:.4f}  F1={f1_cls[0]:.4f}  Support={sup_cls[0]}\")\n",
    "print(f\"Adversarial (1):  Precision={p_cls[1]:.4f}  Recall={r_cls[1]:.4f}  F1={f1_cls[1]:.4f}  Support={sup_cls[1]}\")\n",
    "\n",
    "# Binary score treating class 1 (Adversarial) as the positive class\n",
    "p_bin, r_bin, f1_bin, sup_bin = precision_recall_fscore_support(\n",
    "    all_labels, all_preds, average=\"binary\", pos_label=1, zero_division=0\n",
    ")\n",
    "print(f\"\\nPositive=Adversarial → Precision={p_bin:.4f}  Recall={r_bin:.4f}  F1={f1_bin:.4f}  Support={sup_bin}\")\n",
    "\n",
    "# Averages\n",
    "for avg in [\"macro\", \"weighted\", \"micro\"]:\n",
    "    p, r, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average=avg, zero_division=0\n",
    "    )\n",
    "    print(f\"{avg.capitalize()} avg:   Precision={p:.4f}  Recall={r:.4f}  F1={f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5cfc91",
   "metadata": {},
   "source": [
    "### Senti-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b302967e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "device = \"cuda:1\"\n",
    "model = model.to(device)\n",
    "# === 1. Grad-CAM Generator ===\n",
    "def simple_grad_cam(model, image_tensor, target_class, target_layer):\n",
    "    model.eval()\n",
    "    gradients = []\n",
    "    activations = []\n",
    "\n",
    "    def forward_hook(module, input, output):\n",
    "        activations.append(output)\n",
    "\n",
    "    def backward_hook(module, grad_in, grad_out):\n",
    "        gradients.append(grad_out[0])\n",
    "\n",
    "    handle_fwd = target_layer.register_forward_hook(forward_hook)\n",
    "    handle_bwd = target_layer.register_backward_hook(backward_hook)\n",
    "\n",
    "    output = model(image_tensor)\n",
    "    model.zero_grad()\n",
    "    loss = output[0, target_class]\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    activ = activations[0].detach()\n",
    "    grad = gradients[0].detach()\n",
    "    weights = grad.mean(dim=(2, 3), keepdim=True)\n",
    "    cam = (weights * activ).sum(dim=1, keepdim=True)\n",
    "    cam = F.relu(cam)\n",
    "    cam = F.interpolate(cam, size=image_tensor.shape[-2:], mode='bilinear', align_corners=False)\n",
    "    cam = cam.squeeze()\n",
    "\n",
    "    handle_fwd.remove()\n",
    "    handle_bwd.remove()\n",
    "\n",
    "    return cam\n",
    "\n",
    "# === 2. Patch Overlay ===\n",
    "def overlay_patch(base_image, mask):\n",
    "    patch_color = torch.tensor([1.0, 0.0, 0.0], device=base_image.device).view(3, 1, 1)  # red patch\n",
    "    return base_image * (1 - mask) + patch_color * mask\n",
    "\n",
    "# === 3. SentiNet-style Defense ===\n",
    "\n",
    "# === 3. SentiNet-style Defense (Detector adaptation) ===\n",
    "def sentinet_defense(model, test_loader, benign_images, grad_cam_fn, overlay_fn, target_layer,\n",
    "                     threshold=0.2, device='cuda:1'):\n",
    "    model.eval()\n",
    "\n",
    "    stats_benign = []\n",
    "\n",
    "    print(\"==> Calibrating boundary with benign references...\")\n",
    "    for img in tqdm(benign_images, desc=\"Calibrating\"):\n",
    "        img = img.unsqueeze(0).to(device)\n",
    "        pred_label = model(img).argmax(dim=1).item()\n",
    "        cam = grad_cam_fn(model, img, pred_label, target_layer)\n",
    "        mask = (cam > threshold * cam.max()).float()\n",
    "\n",
    "        fooled_count, inert_conf = 0, []\n",
    "        for ref_img in benign_images:\n",
    "            ref = ref_img.unsqueeze(0).to(device)\n",
    "\n",
    "            # overlay suspect mask into decoy\n",
    "            adv = overlay_fn(ref, mask)\n",
    "\n",
    "            # === STRICT FOOL CRITERION ===\n",
    "            # count as fooled only if decoy is classified as the suspect's predicted class\n",
    "            if model(adv).argmax(dim=1).item() == pred_label:\n",
    "                fooled_count += 1\n",
    "\n",
    "            # inert overlay (noise)\n",
    "            noise_overlay = overlay_fn(ref, torch.randn_like(mask) * 0.01)\n",
    "            conf = F.softmax(model(noise_overlay), dim=1).max().item()\n",
    "            inert_conf.append(conf)\n",
    "\n",
    "        stats_benign.append([fooled_count, np.mean(inert_conf)])\n",
    "\n",
    "    # Fit one-class boundary only on benign references\n",
    "    clf = OneClassSVM(gamma='scale', nu=0.1)\n",
    "    clf.fit(stats_benign)\n",
    "\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    print(\"==> Evaluating on test set...\")\n",
    "    for images, labels, adv_flags in test_loader:\n",
    "        images = images.to(DEVICE)\n",
    "        labels = adv_flags.to(DEVICE)\n",
    "\n",
    "        for i in range(images.size(0)):\n",
    "            img = images[i].unsqueeze(0)\n",
    "            true_label = labels[i].item()\n",
    "            pred_label = model(img).argmax(dim=1).item()\n",
    "\n",
    "            cam = grad_cam_fn(model, img, pred_label, target_layer)\n",
    "            mask = (cam > threshold * cam.max()).float()\n",
    "\n",
    "            fooled_count, inert_conf = 0, []\n",
    "            for ref_img in benign_images:\n",
    "                ref = ref_img.unsqueeze(0).to(device)\n",
    "                adv = overlay_fn(ref, mask)\n",
    "\n",
    "                # === STRICT FOOL CRITERION ===\n",
    "                if model(adv).argmax(dim=1).item() == pred_label:\n",
    "                    fooled_count += 1\n",
    "\n",
    "                noise_overlay = overlay_fn(ref, torch.randn_like(mask) * 0.01)\n",
    "                conf = F.softmax(model(noise_overlay), dim=1).max().item()\n",
    "                inert_conf.append(conf)\n",
    "\n",
    "            test_point = [[fooled_count, np.mean(inert_conf)]]\n",
    "            prediction = clf.predict(test_point)[0]  # -1 = adversarial, 1 = benign\n",
    "\n",
    "            pred_binary = 1 if prediction == -1 else 0\n",
    "            y_true.append(true_label)\n",
    "            y_pred.append(pred_binary)\n",
    "\n",
    "    # === Final Metrics ===\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "\n",
    "    print(f\"\\n✅ SentiNet-Style Detection Metrics (strict criterion):\")\n",
    "    print(f\"Accuracy : {acc * 100:.2f}%\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall   : {recall:.4f}\")\n",
    "    print(f\"F1 Score : {f1:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1910301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a convolutional layer from your model\n",
    "target_layer = dict(model.named_modules())['features.8']  # For ResNet\n",
    "\n",
    "# Choose 10 benign images (label 0) from test_dataset\n",
    "benign_images = [img for img, label in test_dataset if label == 0][:10]\n",
    "\n",
    "\n",
    "results = sentinet_defense(\n",
    "    model=model,\n",
    "    test_loader=test_loader,\n",
    "    benign_images=benign_images,\n",
    "    grad_cam_fn=simple_grad_cam,\n",
    "    overlay_fn=overlay_patch,\n",
    "    target_layer=target_layer,\n",
    "    threshold=0.1, #lower the better - most optimal value \n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Results:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01916edf",
   "metadata": {},
   "source": [
    "### X-Detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5851e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# ---- CONFIG ----\n",
    "DEVICE = 'cuda:1'\n",
    "NUM_CLASSES = 2  # Benign / Adversarial\n",
    "TRANSFORMS = [\n",
    "    lambda x: T.GaussianBlur(kernel_size=5)(x),\n",
    "    lambda x: x + torch.randn_like(x) * 0.05,  # Noise\n",
    "    lambda x: T.ColorJitter(brightness=0.3)(x),\n",
    "    lambda x: T.RandomHorizontalFlip(p=1.0)(x),\n",
    "]\n",
    "\n",
    "# ---- SPD Detector ----\n",
    "def scene_processing_detector(image, model, threshold=0.6):\n",
    "    with torch.no_grad():\n",
    "        orig_pred = F.softmax(model(image.unsqueeze(0).to(DEVICE)), dim=1)[0]\n",
    "\n",
    "        pred_changes = []\n",
    "        for tf in TRANSFORMS:\n",
    "            perturbed = tf(image.clone().cpu())\n",
    "            perturbed_pred = F.softmax(model(perturbed.unsqueeze(0).to(DEVICE)), dim=1)[0]\n",
    "            delta = torch.norm(orig_pred - perturbed_pred).item()\n",
    "            pred_changes.append(delta)\n",
    "\n",
    "        avg_change = np.mean(pred_changes)\n",
    "        is_adv = avg_change > threshold\n",
    "        return is_adv, avg_change\n",
    "\n",
    "# ---- OED Detector (SIFT + KNN, Placeholder using raw pixels) ----\n",
    "def object_extraction_detector(image, model, prototypes, knn_k=3):\n",
    "    sift = cv2.SIFT_create()\n",
    "    img_np = image.permute(1, 2, 0).cpu().detach().numpy()\n",
    "    img_np = (img_np * 255).astype(np.uint8)\n",
    "    kp1, des1 = sift.detectAndCompute(img_np, None)\n",
    "\n",
    "    similarities = []\n",
    "    for label, proto_des in prototypes:\n",
    "        if des1 is None or proto_des is None:\n",
    "            similarities.append((label, 0))\n",
    "            continue\n",
    "        matches = cv2.BFMatcher().knnMatch(des1, proto_des, k=2)\n",
    "        good = [m for m, n in matches if m.distance < 0.75 * n.distance]\n",
    "        similarities.append((label, len(good)))\n",
    "\n",
    "    # KNN voting\n",
    "    similarities.sort(key=lambda x: -x[1])\n",
    "    votes = [lbl for lbl, _ in similarities[:knn_k]]\n",
    "    predicted = Counter(votes).most_common(1)[0][0]\n",
    "\n",
    "    # Get CNN model prediction\n",
    "    with torch.no_grad():\n",
    "        cnn_pred = model(image.unsqueeze(0).to(DEVICE)).argmax(dim=1).item()\n",
    "\n",
    "    is_adv = (predicted != cnn_pred)\n",
    "    return is_adv, predicted, cnn_pred\n",
    "\n",
    "# ---- Evaluation ----\n",
    "def run_xdetect_on_testset(model, test_loader, prototypes):\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for images, labels, adv_flags in test_loader:\n",
    "        for i in range(images.size(0)):\n",
    "            image, label = images[i].to(DEVICE), adv_flags[i].item()\n",
    "\n",
    "            # Run both detectors\n",
    "            spd_flag, _ = scene_processing_detector(image, model)\n",
    "            oed_flag, _, _ = object_extraction_detector(image, model, prototypes)\n",
    "\n",
    "            final_prediction = int(spd_flag or oed_flag)  # 1 = Adversarial, 0 = Benign\n",
    "\n",
    "            y_pred.append(final_prediction)\n",
    "            y_true.append(label)\n",
    "\n",
    "    print(\"=== X-Detect Classification Report ===\")\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"Benign\", \"Adversarial\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e07eec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_sift_prototypes(dataset, num_per_class=5):\n",
    "    sift = cv2.SIFT_create()\n",
    "    prototypes = []\n",
    "\n",
    "    count = 0\n",
    "    for img, label in dataset:\n",
    "        if label == 0:  # Benign class only\n",
    "            img_np = img.permute(1, 2, 0).numpy()\n",
    "            img_np = (img_np * 255).astype(np.uint8)\n",
    "            kp, des = sift.detectAndCompute(img_np, None)\n",
    "            if des is not None:\n",
    "                prototypes.append((label, des))\n",
    "                count += 1\n",
    "        if count >= num_per_class:\n",
    "            break\n",
    "\n",
    "    print(f\"Extracted {count} SIFT prototypes.\")\n",
    "    return prototypes\n",
    "\n",
    "# Example:\n",
    "prototypes = extract_sift_prototypes(test_dataset, num_per_class=10)\n",
    "results = run_xdetect_on_testset(model, test_loader, prototypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf044ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def extract_deep_prototypes(model, dataset, target_layer, device, num=10):\n",
    "    model.eval()\n",
    "    features = []\n",
    "\n",
    "    def hook(module, input, output):\n",
    "        features.append(output.mean(dim=(2, 3)).squeeze().cpu())\n",
    "\n",
    "    handle = target_layer.register_forward_hook(hook)\n",
    "\n",
    "    prototypes = []\n",
    "    count = 0\n",
    "    for img, label in dataset:\n",
    "        if label == 0:  # Only benign samples\n",
    "            features.clear()\n",
    "            model(img.unsqueeze(0).to(device))\n",
    "            if features:\n",
    "                prototypes.append((label, F.normalize(features[0], dim=0)))\n",
    "                count += 1\n",
    "        if count >= num:\n",
    "            break\n",
    "\n",
    "    handle.remove()\n",
    "    print(f\"Extracted {count} deep prototypes.\")\n",
    "    return prototypes\n",
    "\n",
    "\n",
    "target_layer = dict(model.named_modules())['features.8']\n",
    "prototypes = extract_deep_prototypes(model, test_dataset, target_layer, DEVICE, num=10)\n",
    "\n",
    "\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "def deep_feature_oed(model, image, target_layer, prototypes):\n",
    "    model.eval()\n",
    "    features = []\n",
    "\n",
    "    def hook(module, input, output):\n",
    "        features.append(output.mean(dim=(2, 3)).squeeze())\n",
    "\n",
    "    handle = target_layer.register_forward_hook(hook)\n",
    "    _ = model(image.unsqueeze(0))\n",
    "    handle.remove()\n",
    "\n",
    "    test_feat = F.normalize(features[0], dim=0)\n",
    "\n",
    "    similarities = [\n",
    "        cosine_similarity(test_feat, proto_feat.to(test_feat.device), dim=0).item()\n",
    "        for _, proto_feat in prototypes\n",
    "    ]\n",
    "\n",
    "    max_sim = max(similarities)\n",
    "    predicted_match = similarities.index(max_sim)\n",
    "\n",
    "    cnn_pred = model(image.unsqueeze(0)).argmax(dim=1).item()\n",
    "    is_adv = max_sim < 0.85  # threshold — tune if needed\n",
    "\n",
    "    return is_adv, predicted_match, cnn_pred\n",
    "\n",
    "\n",
    "\n",
    "def run_xdetect_on_testset(model, test_loader, prototypes, target_layer):\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for images, labels, adv_flags in test_loader:\n",
    "        for i in range(images.size(0)):\n",
    "            img = images[i].to(DEVICE)\n",
    "            true_label = adv_flags[i].item()\n",
    "\n",
    "            spd_flag, _ = scene_processing_detector(img, model)\n",
    "            oed_flag, _, _ = deep_feature_oed(model, img, target_layer, prototypes)\n",
    "\n",
    "            predicted = int(spd_flag or oed_flag)\n",
    "            y_true.append(true_label)\n",
    "            y_pred.append(predicted)\n",
    "\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(\"=== X-Detect (with Deep OED) Report ===\")\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"Benign\", \"Adversarial\"]))\n",
    "\n",
    "\n",
    "target_layer = dict(model.named_modules())['features.8']\n",
    "prototypes = extract_deep_prototypes(model, test_dataset, target_layer, DEVICE, num=10)\n",
    "\n",
    "# better performing \n",
    "run_xdetect_on_testset(model, test_loader, prototypes, target_layer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
